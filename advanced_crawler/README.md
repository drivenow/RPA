# é«˜æ•ˆçˆ¬è™«æ–¹æ¡ˆ - åŸºäºä»£ç†æµé‡æŠ“åŒ…

è¿™æ˜¯ä¸€ä¸ªåŸºäºä»£ç†æµé‡æŠ“åŒ…çš„é«˜æ•ˆçˆ¬è™«è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ‹¦æˆªç½‘é¡µçš„APIè¯·æ±‚æ¥è·å–ç»“æ„åŒ–æ•°æ®ï¼Œé¿å…äº†ä¼ ç»Ÿçˆ¬è™«è§£æHTMLå…ƒç´ çš„ä¸ç¨³å®šæ€§ã€‚

## ğŸš€ æ ¸å¿ƒä¼˜åŠ¿

1. **é«˜æ•ˆç¨³å®š**: ç›´æ¥è·å–APIè¿”å›çš„ç»“æ„åŒ–æ•°æ®ï¼Œä¸å—é¡µé¢ç»“æ„å˜åŒ–å½±å“
2. **è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜**: ç»“åˆæµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼Œå¯ä»¥æ¨¡æ‹Ÿç”¨æˆ·æ“ä½œè§¦å‘APIè¯·æ±‚
3. **æ•°æ®è´¨é‡å¥½**: è·å–çš„æ˜¯åŸå§‹APIæ•°æ®ï¼Œæ ¼å¼è§„èŒƒï¼Œæ˜“äºå¤„ç†
4. **æ‰©å±•æ€§å¼º**: æ”¯æŒå¤šç§ä»£ç†é…ç½®å’Œè‡ªå®šä¹‰æ•°æ®å¤„ç†é€»è¾‘

## ğŸ“ æ–‡ä»¶ç»“æ„

```
advanced_crawler/
â”œâ”€â”€ proxy_crawler.py          # å®Œæ•´ç‰ˆçˆ¬è™«ï¼ˆåŠŸèƒ½ä¸°å¯Œï¼‰
â”œâ”€â”€ simple_proxy_crawler.py   # ç®€åŒ–ç‰ˆçˆ¬è™«ï¼ˆå¿«é€Ÿä¸Šæ‰‹ï¼‰
â”œâ”€â”€ requirements.txt          # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ README.md                # ä½¿ç”¨è¯´æ˜ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â””â”€â”€ examples/                # ç¤ºä¾‹ä»£ç ï¼ˆå¯é€‰ï¼‰
```

## ğŸ› ï¸ å®‰è£…é…ç½®

### æ–¹æ³•ä¸€ï¼šè‡ªåŠ¨å®‰è£…ï¼ˆæ¨èï¼‰

```bash
# è¿è¡Œè‡ªåŠ¨å®‰è£…è„šæœ¬
python install.py
```

å®‰è£…è„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆä»¥ä¸‹æ“ä½œï¼š
- æ£€æŸ¥Pythonç‰ˆæœ¬å’Œç³»ç»Ÿè¦æ±‚
- å®‰è£…æ‰€æœ‰Pythonä¾èµ–åŒ…
- å®‰è£…Playwrightæµè§ˆå™¨
- åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
- ç”Ÿæˆé»˜è®¤é…ç½®æ–‡ä»¶
- æµ‹è¯•å®‰è£…æ˜¯å¦æˆåŠŸ

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨å®‰è£…

```bash
# 1. å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt

# 2. å®‰è£…Playwrightæµè§ˆå™¨
python -m playwright install

# 3. åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data logs config output temp
```

### 2. éªŒè¯å®‰è£…

```bash
# æµ‹è¯•mitmproxy
mitmdump --version

# æµ‹è¯•playwright
playwright --version
```

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# äº¤äº’å¼é€‰æ‹©é…ç½®
python quick_start.py

# ç›´æ¥æŒ‡å®šé…ç½®
python quick_start.py -c ecommerce    # ç”µå•†çˆ¬è™«
python quick_start.py -c news         # æ–°é—»çˆ¬è™«
python quick_start.py -c test         # æµ‹è¯•çˆ¬è™«

# ä½¿ç”¨ç®€åŒ–ç‰ˆ
python quick_start.py -c test -s

# è®¿é—®è‡ªå®šä¹‰URL
python quick_start.py -u https://example.com

# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é…ç½®
python quick_start.py --list
```

### æ–¹æ³•äºŒï¼šç›´æ¥ä½¿ç”¨ç®€åŒ–ç‰ˆ

```bash
# è¿è¡Œç®€åŒ–ç‰ˆçˆ¬è™«
python simple_proxy_crawler.py
```

ç¨‹åºä¼šï¼š
1. å¯åŠ¨ä»£ç†æœåŠ¡å™¨ï¼ˆé»˜è®¤ç«¯å£8080ï¼‰
2. æ‰“å¼€é…ç½®äº†ä»£ç†çš„æµè§ˆå™¨
3. è‡ªåŠ¨æ‹¦æˆªå¹¶ä¿å­˜æ‰€æœ‰APIå“åº”æ•°æ®

### æ–¹æ³•ä¸‰ï¼šä½¿ç”¨å®Œæ•´ç‰ˆ

```python
from proxy_crawler import AdvancedCrawler
from config_examples import get_config_by_name
import asyncio

async def main():
    # ä½¿ç”¨é¢„å®šä¹‰é…ç½®
    config = get_config_by_name("ecommerce")
    
    # æˆ–è€…è‡ªå®šä¹‰é…ç½®
    # config = CrawlerConfig(
    #     target_domains=["api.example.com"],
    #     target_apis=["/api/data"],
    #     output_dir="./data"
    # )
    
    # åˆ›å»ºçˆ¬è™«
    crawler = AdvancedCrawler(config)
    
    # å®šä¹‰æ“ä½œ
    async def operations(page):
        await page.goto("https://example.com")
        # æ‰§è¡Œæ›´å¤šæ“ä½œ...
    
    # è¿è¡Œ
    await crawler.run(operations)

asyncio.run(main())
```

### æ–¹æ³•å››ï¼šè¿è¡Œç¤ºä¾‹

```bash
# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
python examples.py

# æŸ¥çœ‹é…ç½®ç¤ºä¾‹
python config_examples.py
```

## ğŸ”§ é…ç½®è¯´æ˜

### ä»£ç†é…ç½®

```python
config = CrawlerConfig(
    proxy_type="mitmproxy",     # ä»£ç†ç±»å‹
    proxy_host="127.0.0.1",    # ä»£ç†ä¸»æœº
    proxy_port=8080,           # ä»£ç†ç«¯å£
    
    # ç›®æ ‡è¿‡æ»¤
    target_domains=[           # è¦æ‹¦æˆªçš„åŸŸå
        "api.example.com",
        "data.website.com"
    ],
    target_apis=[              # è¦æ‹¦æˆªçš„APIè·¯å¾„
        "/api/",
        "/data/",
        "/v1/"
    ],
    
    # æµè§ˆå™¨é…ç½®
    headless=False,            # æ˜¯å¦æ— å¤´æ¨¡å¼
    browser_type="chromium",   # æµè§ˆå™¨ç±»å‹
    
    # æ•°æ®å­˜å‚¨
    output_dir="./output",     # è¾“å‡ºç›®å½•
    save_raw_data=True,        # ä¿å­˜åŸå§‹æ•°æ®
    save_parsed_data=True,     # ä¿å­˜è§£æåæ•°æ®
    
    # æ€§èƒ½é…ç½®
    max_concurrent=5,          # æœ€å¤§å¹¶å‘æ•°
    request_delay=1.0,         # è¯·æ±‚é—´éš”
    timeout=30                 # è¶…æ—¶æ—¶é—´
)
```

### è‡ªå®šä¹‰æ•°æ®å¤„ç†

```python
class CustomDataProcessor(DataProcessor):
    def _parse_data(self, data):
        """è‡ªå®šä¹‰æ•°æ®è§£æé€»è¾‘"""
        if data.get("type") != "response":
            return None
            
        response_data = data.get("data")
        if not isinstance(response_data, dict):
            return None
            
        # æå–ç‰¹å®šå­—æ®µ
        extracted = {
            "timestamp": data.get("timestamp"),
            "url": data.get("url"),
            "items": response_data.get("items", []),
            "total": response_data.get("total", 0),
            "page": response_data.get("page", 1)
        }
        
        return extracted
```

## ğŸ“Š ä½¿ç”¨åœºæ™¯

### 1. ç”µå•†æ•°æ®é‡‡é›†

```python
# é…ç½®ç”µå•†ç½‘ç«™APIæ‹¦æˆª
config = CrawlerConfig(
    target_domains=["api.shop.com"],
    target_apis=["/api/products", "/api/search"],
    output_dir="./ecommerce_data"
)

# è‡ªå®šä¹‰æ“ä½œï¼šæœç´¢å•†å“
async def search_products(page):
    await page.fill("input[name='search']", "æ‰‹æœº")
    await page.click("button[type='submit']")
    await page.wait_for_load_state("networkidle")
```

### 2. ç¤¾äº¤åª’ä½“æ•°æ®

```python
# é…ç½®ç¤¾äº¤åª’ä½“APIæ‹¦æˆª
config = CrawlerConfig(
    target_domains=["api.social.com"],
    target_apis=["/api/posts", "/api/comments"],
    output_dir="./social_data"
)

# è‡ªå®šä¹‰æ“ä½œï¼šæ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
async def scroll_for_more(page):
    for i in range(5):
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(2)
```

### 3. æ–°é—»èµ„è®¯é‡‡é›†

```python
# é…ç½®æ–°é—»ç½‘ç«™APIæ‹¦æˆª
config = CrawlerConfig(
    target_domains=["api.news.com"],
    target_apis=["/api/articles", "/api/categories"],
    output_dir="./news_data"
)
```

## ğŸ” æ•°æ®è¾“å‡ºæ ¼å¼

### åŸå§‹æ•°æ®æ ¼å¼

```json
{
  "timestamp": "2024-01-15T10:30:45.123456",
  "url": "https://api.example.com/data",
  "method": "GET",
  "status_code": 200,
  "headers": {
    "content-type": "application/json"
  },
  "type": "response",
  "data": {
    "items": [...],
    "total": 100,
    "page": 1
  }
}
```

### è§£æåæ•°æ®æ ¼å¼

```json
{
  "timestamp": "2024-01-15T10:30:45.123456",
  "url": "https://api.example.com/data",
  "status_code": 200,
  "method": "GET",
  "extracted_data": [
    {"id": 1, "title": "Item 1"},
    {"id": 2, "title": "Item 2"}
  ]
}
```

## ğŸ›¡ï¸ ä»£ç†é…ç½®æ–¹æ¡ˆ

### æ–¹æ¡ˆä¸€ï¼šmitmproxyï¼ˆæ¨èï¼‰

**ä¼˜ç‚¹**ï¼š
- PythonåŸç”Ÿæ”¯æŒï¼Œé›†æˆåº¦é«˜
- åŠŸèƒ½å¼ºå¤§ï¼Œæ”¯æŒHTTPS
- å¯ç¼–ç¨‹æ€§å¼º

**é…ç½®**ï¼š
```python
# ç¨‹åºä¼šè‡ªåŠ¨å¯åŠ¨mitmproxyä»£ç†æœåŠ¡å™¨
# æµè§ˆå™¨ä¼šè‡ªåŠ¨é…ç½®ä½¿ç”¨è¯¥ä»£ç†
```

### æ–¹æ¡ˆäºŒï¼šç³»ç»Ÿä»£ç† + Proxifier

**ä¼˜ç‚¹**ï¼š
- å¯ä»¥ä»£ç†æ‰€æœ‰åº”ç”¨ç¨‹åºçš„æµé‡
- æ”¯æŒæ›´å¤æ‚çš„è·¯ç”±è§„åˆ™

**é…ç½®æ­¥éª¤**ï¼š
1. ä¸‹è½½å®‰è£…Proxifier
2. é…ç½®ä»£ç†æœåŠ¡å™¨ï¼š127.0.0.1:8080
3. è®¾ç½®ä»£ç†è§„åˆ™ï¼ŒæŒ‡å®šè¦ä»£ç†çš„åº”ç”¨ç¨‹åº
4. è¿è¡Œçˆ¬è™«ç¨‹åº

### æ–¹æ¡ˆä¸‰ï¼šæµè§ˆå™¨æ’ä»¶ä»£ç†

**ä¼˜ç‚¹**ï¼š
- é…ç½®ç®€å•
- åªä»£ç†æµè§ˆå™¨æµé‡

**é…ç½®æ­¥éª¤**ï¼š
1. å®‰è£…SwitchyOmegaç­‰ä»£ç†æ’ä»¶
2. é…ç½®ä»£ç†ï¼š127.0.0.1:8080
3. å¯ç”¨ä»£ç†é…ç½®
4. è¿è¡Œçˆ¬è™«ç¨‹åº

## ğŸš¨ å¸¸è§é—®é¢˜

### Q1: SSLè¯ä¹¦é”™è¯¯

**é—®é¢˜**ï¼šè®¿é—®HTTPSç½‘ç«™æ—¶å‡ºç°è¯ä¹¦é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å®‰è£…mitmproxyè¯ä¹¦
# 1. å¯åŠ¨ä»£ç†åè®¿é—® http://mitm.it
# 2. ä¸‹è½½å¹¶å®‰è£…å¯¹åº”å¹³å°çš„è¯ä¹¦
# 3. æˆ–è€…åœ¨ä»£ç ä¸­è®¾ç½®å¿½ç•¥SSLé”™è¯¯
opts = options.Options(ssl_insecure=True)
```

### Q2: ä»£ç†ç«¯å£è¢«å ç”¨

**é—®é¢˜**ï¼šç«¯å£8080å·²è¢«å…¶ä»–ç¨‹åºä½¿ç”¨

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ›´æ”¹ä»£ç†ç«¯å£
config = CrawlerConfig(proxy_port=8081)  # ä½¿ç”¨å…¶ä»–ç«¯å£
```

### Q3: æµè§ˆå™¨æ— æ³•è¿æ¥ä»£ç†

**é—®é¢˜**ï¼šæµè§ˆå™¨æ˜¾ç¤ºæ— æ³•è¿æ¥åˆ°ä»£ç†æœåŠ¡å™¨

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
2. ç¡®è®¤ä»£ç†æœåŠ¡å™¨å·²å¯åŠ¨
3. æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®

### Q4: æ•°æ®æ²¡æœ‰è¢«æ‹¦æˆª

**é—®é¢˜**ï¼šè®¿é—®ç½‘ç«™ä½†æ²¡æœ‰æ‹¦æˆªåˆ°APIæ•°æ®

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥target_domainså’Œtarget_apisé…ç½®
2. ç¡®è®¤ç½‘ç«™ç¡®å®ä½¿ç”¨äº†APIè¯·æ±‚
3. æŸ¥çœ‹æ—¥å¿—è¾“å‡ºï¼Œç¡®è®¤è¯·æ±‚è¢«æ­£ç¡®æ‹¦æˆª

### Q5: æ€§èƒ½é—®é¢˜

**é—®é¢˜**ï¼šçˆ¬è™«è¿è¡Œç¼“æ…¢æˆ–å ç”¨èµ„æºè¿‡å¤š

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä¼˜åŒ–é…ç½®
config = CrawlerConfig(
    max_concurrent=3,      # é™ä½å¹¶å‘æ•°
    request_delay=2.0,     # å¢åŠ è¯·æ±‚é—´éš”
    headless=True,         # ä½¿ç”¨æ— å¤´æ¨¡å¼
    save_raw_data=False    # åªä¿å­˜è§£æåçš„æ•°æ®
)
```

## ğŸ“ˆ é«˜çº§åŠŸèƒ½

### 1. æ•°æ®å»é‡

```python
class DeduplicatedDataProcessor(DataProcessor):
    def __init__(self, config):
        super().__init__(config)
        self.seen_urls = set()
    
    def process_data(self, data):
        url = data.get("url")
        if url in self.seen_urls:
            return None  # è·³è¿‡é‡å¤æ•°æ®
        
        self.seen_urls.add(url)
        return super().process_data(data)
```

### 2. æ•°æ®è¿‡æ»¤

```python
def custom_filter(data):
    """è‡ªå®šä¹‰æ•°æ®è¿‡æ»¤å™¨"""
    response_data = data.get("data", {})
    
    # åªä¿å­˜åŒ…å«ç‰¹å®šå­—æ®µçš„æ•°æ®
    if "items" not in response_data:
        return False
    
    # åªä¿å­˜éç©ºæ•°æ®
    if not response_data.get("items"):
        return False
    
    return True
```

### 3. å®æ—¶æ•°æ®å¤„ç†

```python
import asyncio
from datetime import datetime

class RealTimeProcessor:
    def __init__(self):
        self.data_buffer = []
    
    async def process_realtime(self, data):
        """å®æ—¶å¤„ç†æ•°æ®"""
        self.data_buffer.append(data)
        
        # æ¯æ”¶é›†10æ¡æ•°æ®å¤„ç†ä¸€æ¬¡
        if len(self.data_buffer) >= 10:
            await self.batch_process()
            self.data_buffer.clear()
    
    async def batch_process(self):
        """æ‰¹é‡å¤„ç†æ•°æ®"""
        # æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€å­˜å‚¨ç­‰æ“ä½œ
        processed_data = []
        for item in self.data_buffer:
            # å¤„ç†é€»è¾‘
            processed_item = self.transform_data(item)
            processed_data.append(processed_item)
        
        # ä¿å­˜åˆ°æ•°æ®åº“æˆ–æ–‡ä»¶
        await self.save_to_database(processed_data)
```

## ğŸ”— æ‰©å±•é›†æˆ

### ä¸æ•°æ®åº“é›†æˆ

```python
import asyncpg  # PostgreSQL
import aiomysql  # MySQL

class DatabaseIntegration:
    async def save_to_postgres(self, data):
        conn = await asyncpg.connect(
            host='localhost',
            database='crawler_db',
            user='user',
            password='password'
        )
        
        await conn.execute(
            "INSERT INTO api_data (url, data, timestamp) VALUES ($1, $2, $3)",
            data['url'], json.dumps(data['data']), data['timestamp']
        )
        
        await conn.close()
```

### ä¸æ¶ˆæ¯é˜Ÿåˆ—é›†æˆ

```python
import aio_pika  # RabbitMQ

class MessageQueueIntegration:
    async def send_to_queue(self, data):
        connection = await aio_pika.connect_robust("amqp://localhost/")
        channel = await connection.channel()
        
        await channel.default_exchange.publish(
            aio_pika.Message(json.dumps(data).encode()),
            routing_key="crawler_data"
        )
        
        await connection.close()
```

## ğŸ“ æœ€ä½³å®è·µ

1. **åˆç†è®¾ç½®è¯·æ±‚é—´éš”**ï¼šé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆè¿‡å¤§å‹åŠ›
2. **ä½¿ç”¨æ•°æ®å»é‡**ï¼šé¿å…é‡å¤é‡‡é›†ç›¸åŒæ•°æ®
3. **ç›‘æ§èµ„æºä½¿ç”¨**ï¼šå®šæœŸæ£€æŸ¥å†…å­˜å’Œç£ç›˜ä½¿ç”¨æƒ…å†µ
4. **é”™è¯¯å¤„ç†**ï¼šå®ç°å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
5. **æ—¥å¿—è®°å½•**ï¼šè®°å½•è¯¦ç»†çš„è¿è¡Œæ—¥å¿—ä¾¿äºè°ƒè¯•
6. **éµå®ˆrobots.txt**ï¼šå°Šé‡ç½‘ç«™çš„çˆ¬è™«åè®®
7. **æ•°æ®å¤‡ä»½**ï¼šå®šæœŸå¤‡ä»½é‡è¦çš„é‡‡é›†æ•°æ®

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

## ğŸ“ æ”¯æŒ

å¦‚æœæ‚¨åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ï¼š
1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„å¸¸è§é—®é¢˜éƒ¨åˆ†
2. æäº¤GitHub Issue
3. æŸ¥çœ‹mitmproxyå’Œplaywrightçš„å®˜æ–¹æ–‡æ¡£